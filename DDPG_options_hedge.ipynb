{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN29eXnhnlBgTyunLhxXw7L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newmantic/DDPG_options_hedge/blob/main/DDPG_options_hedge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQ9zZTK1glyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rPCIS-Asb8ES"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import gym\n",
        "\n",
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_dev, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_dev\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x_initial if self.x_initial is not None else np.zeros_like(self.mean)\n",
        "\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, num_states, num_actions, buffer_capacity=100000, batch_size=64):\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Correct the shape to match the environment's state and action spaces\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    def record(self, obs_tuple):\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "\n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def sample(self):\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPGAgent:\n",
        "    def __init__(self, num_states, num_actions, upper_bound, lower_bound):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.upper_bound = upper_bound\n",
        "        self.lower_bound = lower_bound\n",
        "\n",
        "        self.actor_model = self.create_actor_model()\n",
        "        self.critic_model = self.create_critic_model()\n",
        "\n",
        "        self.target_actor = self.create_actor_model()\n",
        "        self.target_critic = self.create_critic_model()\n",
        "\n",
        "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
        "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
        "\n",
        "        # Initialize the buffer with the correct state and action dimensions\n",
        "        self.buffer = Buffer(num_states, num_actions)\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.critic_lr = 0.002\n",
        "        self.actor_lr = 0.001\n",
        "\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(self.critic_lr)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(self.actor_lr)\n",
        "\n",
        "        self.noise = OUActionNoise(np.zeros(1), std_dev=0.2)\n",
        "\n",
        "    def create_actor_model(self):\n",
        "        inputs = layers.Input(shape=(self.num_states,))\n",
        "        out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        out = layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = layers.Dense(self.num_actions, activation=\"tanh\")(out)\n",
        "\n",
        "        model = tf.keras.Model(inputs, outputs)\n",
        "        return model\n",
        "\n",
        "    def create_critic_model(self):\n",
        "        state_input = layers.Input(shape=(self.num_states,))\n",
        "        action_input = layers.Input(shape=(self.num_actions,))\n",
        "\n",
        "        concat = layers.Concatenate()([state_input, action_input])\n",
        "\n",
        "        out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "        out = layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = layers.Dense(1)(out)\n",
        "\n",
        "        model = tf.keras.Model([state_input, action_input], outputs)\n",
        "        return model\n",
        "\n",
        "    def policy(self, state):\n",
        "        sampled_actions = tf.squeeze(self.actor_model(state))\n",
        "        noise = self.noise()\n",
        "        sampled_actions = sampled_actions.numpy() + noise\n",
        "        legal_action = np.clip(sampled_actions, self.lower_bound, self.upper_bound)\n",
        "        return np.squeeze(legal_action)\n",
        "\n",
        "    def update(self):\n",
        "        state_batch, action_batch, reward_batch, next_state_batch = self.buffer.sample()\n",
        "\n",
        "        # Ensure the reward_batch is of type float32\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = self.target_actor(next_state_batch)\n",
        "\n",
        "            # Cast critic's output to float32 to ensure same type for addition\n",
        "            target_critic_value = self.target_critic([next_state_batch, target_actions], training=True)\n",
        "            y = reward_batch + self.gamma * tf.cast(target_critic_value, dtype=tf.float32)\n",
        "\n",
        "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic_model.trainable_variables))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor_model(state_batch, training=True)\n",
        "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor_model.trainable_variables))\n",
        "\n",
        "    def update_target(self, target_weights, weights):\n",
        "        for (a, b) in zip(target_weights, weights):\n",
        "            a.assign(b * self.tau + a * (1 - self.tau))\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.update_target(self.target_actor.variables, self.actor_model.variables)\n",
        "        self.update_target(self.target_critic.variables, self.critic_model.variables)\n",
        "\n"
      ],
      "metadata": {
        "id": "64LYC523iqJn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ddpg():\n",
        "    env = gym.make(\"Pendulum-v1\")  # Replace with an options trading simulator environment\n",
        "    num_states = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.shape[0]\n",
        "    upper_bound = env.action_space.high[0]\n",
        "    lower_bound = env.action_space.low[0]\n",
        "\n",
        "    agent = DDPGAgent(num_states, num_actions, upper_bound, lower_bound)\n",
        "\n",
        "    episodes = 30\n",
        "    for ep in range(episodes):\n",
        "        prev_state = env.reset()\n",
        "        episodic_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # The action returned by agent.policy is a single value array, so we need to extract it.\n",
        "            action = agent.policy(tf.convert_to_tensor([prev_state], dtype=tf.float32))\n",
        "            action = np.squeeze(action)  # Extract the scalar value\n",
        "\n",
        "            # Pass the scalar action to env.step\n",
        "            state, reward, done, _ = env.step([action])  # Ensure action is passed as a list/array\n",
        "\n",
        "            agent.buffer.record((prev_state, [action], reward, state))\n",
        "            episodic_reward += reward\n",
        "\n",
        "            agent.update()\n",
        "            agent.update_target_network()\n",
        "\n",
        "            prev_state = state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        print(f\"Episode {ep}, Reward: {episodic_reward}\")"
      ],
      "metadata": {
        "id": "pZ_5LY-Wh5aV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_ddpg()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmJShAp-gl6N",
        "outputId": "336834b7-0244-4d52-de2b-7adc94edf95c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Reward: -1251.0065181114087\n",
            "Episode 1, Reward: -1340.8269607252926\n",
            "Episode 2, Reward: -1750.1570692085372\n",
            "Episode 3, Reward: -1486.8933472192468\n",
            "Episode 4, Reward: -1616.5251307690714\n",
            "Episode 5, Reward: -1415.7678569126706\n",
            "Episode 6, Reward: -1359.1714614929513\n",
            "Episode 7, Reward: -1212.1008949267286\n",
            "Episode 8, Reward: -1639.5173224021191\n",
            "Episode 9, Reward: -1061.4995448931588\n",
            "Episode 10, Reward: -1170.1961428302375\n",
            "Episode 11, Reward: -902.983293815275\n",
            "Episode 12, Reward: -931.3447052498097\n",
            "Episode 13, Reward: -372.43118919121963\n",
            "Episode 14, Reward: -1155.7972916245972\n",
            "Episode 15, Reward: -247.27060395362665\n",
            "Episode 16, Reward: -241.96959259250056\n",
            "Episode 17, Reward: -1054.002444442528\n",
            "Episode 18, Reward: -823.7997258548645\n",
            "Episode 19, Reward: -244.54180496750422\n",
            "Episode 20, Reward: -787.9266572693169\n",
            "Episode 21, Reward: -505.4232198651289\n",
            "Episode 22, Reward: -127.34765836924323\n",
            "Episode 23, Reward: -496.762125960873\n",
            "Episode 24, Reward: -377.56677103874745\n",
            "Episode 25, Reward: -493.4580314752547\n",
            "Episode 26, Reward: -715.3965376024247\n",
            "Episode 27, Reward: -378.21015848370814\n",
            "Episode 28, Reward: -123.23078152642377\n",
            "Episode 29, Reward: -630.4928200444965\n"
          ]
        }
      ]
    }
  ]
}